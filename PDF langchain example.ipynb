{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain sentence_transformers InstructorEmbedding pypdf chromadb llama-cpp-python openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import LlamaCpp, OpenAI, TextGen\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### documents loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory:str = './Documents'\n",
    "os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this after uploading the PDFs to the directory\n",
    "\n",
    "PDFloader = DirectoryLoader(directory, glob='./*.pdf', loader_cls=PyPDFLoader)\n",
    "Textloader = DirectoryLoader(directory, glob='./*.txt', loader_cls=TextLoader)\n",
    "\n",
    "documents = PDFloader.load()+Textloader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### document splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size: int = 512 # use value from 0 to 512 \n",
    "chunk_overlap: int = 0 # use value from 0 to 512 \n",
    "    \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "chunks = text_splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cpu' #for cuda need to install torch compiled with cuda \n",
    "\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={'device': device})\n",
    "\n",
    "# model_name = 'hkunlp/instructor-base'\n",
    "# embeddings = HuggingFaceInstructEmbeddings(model_name=model_name, model_kwargs={'device': device})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k: int = 5 # number of chunks to retrieve\n",
    "persist_directory: str = './index'\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=persist_directory)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever.search_kwargs[\"k\"] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model download for llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id, name = 'TheBloke/StableBeluga-13B-GGML', 'stablebeluga-13b.ggmlv3.q2_K.bin'\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs('./Models', exist_ok=True)\n",
    "filename = f'./Models/{name}'\n",
    "if not os.path.isfile(filename):\n",
    "    url = f'https://huggingface.co/{repo_id}/resolve/main/{name}'\n",
    "    urllib.request.urlretrieve(url=url, filename=filename)\n",
    "    print(\"File downloaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### llm parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature: float = 0.1\n",
    "top_p: float = 0.1\n",
    "max_tokens: int = 2048\n",
    "top_k: int = 40\n",
    "stopping_strings = ['### System:', '### User:', '\\n\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "textgen = TextGen(\n",
    "    model_url='http://127.0.0.1:5000', # must use public-api in textgen webui\n",
    "    temperature=temperature,\n",
    "    max_new_tokens=max_tokens,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    stopping_strings=stopping_strings,\n",
    "    )\n",
    "\n",
    "openai = OpenAI(\n",
    "                model_name='text-davinci-003',\n",
    "                openai_api_base='https://api.openai.com/v1',\n",
    "                openai_api_key='sk-111111111111111111111111111111111111111111111111',\n",
    "                streaming=False,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                top_p=top_p,\n",
    "                )\n",
    "\n",
    "llamacpp = LlamaCpp(\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                stop=stopping_strings,\n",
    "                model_path=filename,\n",
    "               )\n",
    "\n",
    "llm = llamacpp # textgen, openai, llamacpp, change this to switch llm backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerConversationBufferMemory(ConversationBufferMemory):\n",
    "    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:\n",
    "        return super(AnswerConversationBufferMemory, self).save_context(inputs,{'response': outputs['answer']})\n",
    "    \n",
    "memory = AnswerConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template: Orca-Hashes\n",
    "\n",
    "sys_name = '### System:'\n",
    "user_name = '### User:'\n",
    "input_name = '### Input:'\n",
    "res_name = '### Response:'\n",
    "\n",
    "\n",
    "system = 'You are an AI assistant that follows instruction extremely well. Help as much as you can.'\n",
    "\n",
    "# condense question prompt\n",
    "\n",
    "user = 'Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.'\n",
    "input_text = 'Chat History:\\n{chat_history}\\nFollow up question: {question}'\n",
    "response = 'Standalone question:'\n",
    "\n",
    "condense_question_prompt = PromptTemplate.from_template(template=f'{sys_name}\\n{system}\\n\\n{user_name}\\n{user}\\n\\n{input_name}\\n{input_text}\\n\\n{res_name}\\n{response}')\n",
    "\n",
    "# qa prompt\n",
    "user = '''Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.'''\n",
    "input_text = 'Question: {question}\\nPieces of context:\\n{context}'\n",
    "response = 'Helpful Answer:'\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(template=f'{sys_name}\\n{system}\\n\\n{user_name}\\n{user}\\n\\n{input_name}\\n{input_text}\\n\\n{res_name}\\n{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm,\n",
    "            retriever=retriever,\n",
    "            condense_question_prompt=condense_question_prompt,\n",
    "            return_source_documents=True,\n",
    "            combine_docs_chain_kwargs={\"prompt\": qa_prompt},\n",
    "            rephrase_question = True,\n",
    "            memory=memory,\n",
    "            verbose=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(res):\n",
    "    answer = res[\"answer\"]\n",
    "    source_documents = {}\n",
    "\n",
    "    for document in res['source_documents']:\n",
    "        page_content = document.page_content\n",
    "        source = document.metadata['source']\n",
    "        page = document.metadata['page']\n",
    "        document_string = f'contenido: \"{page_content}\"'\n",
    "        if source not in source_documents:\n",
    "            source_documents[source] = {}\n",
    "        source_documents[source][page] = document_string\n",
    "\n",
    "    return answer, source_documents\n",
    "\n",
    "def query(prompt: str, chain):\n",
    "    res = chain({\"question\" : prompt})\n",
    "    answer, source_documents = process_response(res=res)\n",
    "    return answer, source_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt :str = input('query: ')\n",
    "answer, source_documents = query(prompt=prompt, chain=chain)\n",
    "print(\"Response:\",\"\\n\",answer,\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
